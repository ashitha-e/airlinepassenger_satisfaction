# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of customer satisfaction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_dTYhXULo9dbRWRq85l8uXOigq1GIdNR
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
path = "/content/drive/MyDrive/Exam results/test data"
train = pd.read_excel(path)
train.describe

import pandas as pd
path1 = "/content/drive/MyDrive/Exam results/train"
test = pd.read_excel(path1)
test.describe

df_ori = pd.concat([train, test], sort=False)
df = df_ori.copy()
df = df.iloc[:,8:24]
df = df.dropna()
df

df.describe().transpose()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df.head()

df.isnull()

df.isnull().sum()

df.duplicated()

sns.set_style("whitegrid")

train.head()

if sum(train.columns!=test.columns) == 0:
    print ("match")
else:
    print("error")

print("Test dataset is",(test.shape[0]/(train.shape[0]+test.shape[0]))*100,"% of the train dataset")

train.info()

train.isnull().sum().reset_index(name = "Null values").set_index("index")

test.isnull().sum().reset_index(name = "Null values").set_index("index")

train.drop("Unnamed: 0", axis = 1, inplace = True)
test.drop("Unnamed: 0", axis = 1, inplace = True)

def pivot_satisfaction (train, target):

    df_pivot = pd.pivot_table(
        train[["id", target, "satisfaction"]],
        index = [target],
        columns = ["satisfaction"],
        aggfunc = "count",
        fill_value =0).reset_index()

    df_pivot.columns = [target, "neutral or dissatisfied", "satisfied"]

    df_pivot["total"] = df_pivot["satisfied"]+df_pivot["neutral or dissatisfied"]
    df_pivot["dissatisfied_rate"] = df_pivot["neutral or dissatisfied"]/df_pivot["total"]*100

    return df_pivot



plt.figure(figsize = (8,5))
sns.countplot(x ="satisfaction", data = train, hue ="satisfaction", palette ="Pastel1" )


sns.despine(top = True, right = True, left = False, bottom = False)
plt.title("Satisfaction results")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),  title = "satisfaction")

plt.show()

print(round((train[train["satisfaction"]=="neutral or dissatisfied"].shape[0]/train.shape[0])*100, 2),"% of the passenger were neutral or dissatisfied with the service provided by the airline")

satisfaction_dummmy = pd.get_dummies(train["satisfaction"], drop_first = True)
train = pd.concat([train, satisfaction_dummmy], axis=1)

#gender
plt.figure(figsize = (8,5))
sns.countplot(x ="Gender", data = train, hue ="satisfaction", palette ="Pastel1" )


sns.despine(top = True, right = True, left = False, bottom = False)
plt.title("Satisfaction results by gender")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),  title = "satisfaction")

plt.show()

pivot_satisfaction(train,"Gender")

Gender_dummmy = pd.get_dummies(train["Gender"], drop_first = True)
train = pd.concat([train, Gender_dummmy], axis=1)

Gender_dummmy = pd.get_dummies(test["Gender"], drop_first = True)
test = pd.concat([test, Gender_dummmy], axis=1)

plt.figure(figsize = (8,5))
sns.countplot(x ="Customer Type", data = train, hue ="satisfaction" ,palette ="Pastel1" )


plt.title("Satisfaction results by Customer Type")
sns.despine(top = True, right = True, left = False, bottom = False)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),  title = "satisfaction")

plt.show()

pivot_satisfaction(train,"Customer Type")

Customer_dummmy = pd.get_dummies(train["Customer Type"], drop_first = True)

train = pd.concat([train, Customer_dummmy], axis=1)

plt.figure(figsize = (8,5))
sns.histplot( x= "Age", data = train, kde= True)

plt.title("Distribution of age")
plt.show()

plt.figure(figsize = (8,5))
sns.kdeplot(data = train, x= "Age", hue = "satisfaction", common_norm = False, palette ="RdPu_r")

plt.title("Satisfaction results by age")
sns.despine(top = True, right = True, left = False, bottom = False)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),  title = "satisfaction")
plt.axvline(train["Age"].mean(),ls = "--", color = "red")
plt.text(train["Age"].mean()-2,0,'average',rotation=90)
plt.show()

train["Age"].mean()

def age_group (row):
    if row < 40:
        return "Young"
    if (row >= 40) & (row < 60):
        return "middle age"
    if row >= 60:
        return "senior"
    else:
        return "no age"

train["Age_group"] = train["Age"].apply(lambda row: age_group(row))

pivot_satisfaction(train,"Age_group")

plt.figure(figsize = (8,5))
sns.boxplot(x = "Age", data = test,palette ="Pastel1" )

plt.title("Distrubution of age for test data")
sns.despine(top = True, right = True, left = False, bottom = False)
plt.show()

plt.figure(figsize = (8,5))
sns.countplot(x ="Type of Travel", data = train, hue ="satisfaction",palette ="Pastel1" )

plt.title("Satisfaction results by Type of Travel")
sns.despine(top = True, right = True, left = False, bottom = False)
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5),  title = "satisfaction")

plt.show()

pivot_satisfaction(train,"Type of Travel")

Type_dummmy = pd.get_dummies(train["Type of Travel"], drop_first = True)

train = pd.concat([train, Type_dummmy], axis=1)

#classes

#factor analysis

pip install factor_analyzer # installing Factor Analyzer

from factor_analyzer import FactorAnalyzer # Then, we import the installed package into our notebook.

from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
calculate_bartlett_sphericity(df)

from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all, kmo_model = calculate_kmo(df)
print(kmo_model)

# instantiate the Factor Analyzer
fa = FactorAnalyzer()

# Fit the dataframe using Factor Analyzer
fa.fit(df)

# Identify the eigenvalues
ev, v = fa.get_eigenvalues() #eigenvalues

# display the eigenvalues
ev

# Then, we repeat the factor analyzer using five factors, fitting it, and print the factor loadings for each variables.
fa = FactorAnalyzer(5, rotation='varimax')
fa.fit(df)
print(fa.loadings_)

lmatrix = pd.DataFrame(fa.loadings_, index = list(df.columns), columns = ['Factor 1', 'Factor 2', 'Factor 3', 'Factor 4', 'Factor 5'])
lmatrix #loading matrix

lmatrix.sort_values('Factor 1', ascending=False)

lmatrix.sort_values('Factor 2', ascending=False)

lmatrix.sort_values('Factor 3', ascending=False)

lmatrix.sort_values('Factor 4', ascending=False)

lmatrix.sort_values('Factor 5', ascending=False)

fa.get_factor_variance()

#Factor 1: Cleanliness, Food and Drink, Inflight Entertainment, Seat Comfort
#Factor 2: Inflight Services, Baggage Handling, Onboard Services, Leg Room
#Factor 3: Arrival and Departure Delay
#Factor 4: Online Booking, Gate Location, Inflight wifi, Departure/Arrival Time Convenience
#Factor 5: Online Boarding

#LOGISTIC REGRESSION

import pandas as pd
path = "/content/drive/MyDrive/Exam results/test data"
df = pd.read_excel(path)
df.describe

df.head()

df.columns

y_col = ["satisfaction"]
useless_cols = ["Unnamed: 0" , "id"]
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
newdf = df.select_dtypes(include=numerics)
numeric_cols = [f for f in newdf.columns if f not in useless_cols]
cat_cols = list(set(df.columns).difference(set(useless_cols).union(set(numeric_cols))))
cat_cols = [c for c in cat_cols if c not in y_col]

numeric_cols

cat_cols

encoded_cat_df = pd.get_dummies(df[cat_cols] , drop_first =True)

X = encoded_cat_df.join(df[numeric_cols])
#X= encoded_cat_df
y = df[y_col]

X

X.loc[: , numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].mean())

import statsmodels.api as sm

X = sm.add_constant(X)

np.where(y == "satisfied" , 1, 0)

log_reg = sm.Logit(np.where(y == "satisfied" , 1, 0), X).fit_regularized()

log_reg.summary()

np.exp(log_reg.params)

X_train = train.drop('satisfaction', axis=1)
y_train = train['satisfaction']

X_test = test.drop('satisfaction', axis=1)
y_test = test['satisfaction']

X_train.head()

#the way

path = "/content/drive/MyDrive/Exam results/test data"
train = pd.read_excel(path)

train = train.drop('id',axis=1)
X=train.iloc[:,:1].values
y=train.iloc[:,4].values

#org way

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix

from tabulate import tabulate

import warnings
pd.set_option('display.max_columns', None)
warnings.filterwarnings('ignore')

test = pd.read_excel(path1)
test.describe

test.head()

test=test.drop(['Unnamed: 0','id'],axis=1)

tset=test.dropna()

labelEncoder=LabelEncoder()
test['satisfaction']=labelEncoder.fit_transform(test[['satisfaction']])

print({l: i for i, l in enumerate(labelEncoder.classes_)})

encoderG=OneHotEncoder(sparse=False)
testG=pd.DataFrame(encoderG.fit_transform(test[['Gender']]))
testG.columns = encoderG.get_feature_names_out(['Gender'])
testG=testG.drop(['Gender_Male'],axis=1)

encoderC=OneHotEncoder(sparse=False)
testC=pd.DataFrame(encoderC.fit_transform(test[['Customer Type']]))
testC.columns = encoderC.get_feature_names_out(['Customer Type'])
testC=testC.drop(['Customer Type_disloyal Customer'],axis=1)

encoderT=OneHotEncoder(sparse=False)
testT=pd.DataFrame(encoderT.fit_transform(test[['Type of Travel']]))
testT.columns = encoderT.get_feature_names_out(['Type of Travel'])
testT=testT.drop(['Type of Travel_Personal Travel'],axis=1)

encoderCl=OneHotEncoder(sparse=False)
testCl=pd.DataFrame(encoderCl.fit_transform(test[['Class']]))
testCl.columns = encoderCl.get_feature_names_out(['Class'])
testCl=testCl.drop(['Class_Eco Plus'],axis=1)

test=test.drop(['Gender','Customer Type','Type of Travel','Class'],axis=1)

test= pd.concat([testG, testC, testT, testCl, test ], axis=1)

test=test.dropna()

standardScalerA=StandardScaler()
test['Age']=standardScalerA.fit_transform(test[['Age']])

standardScalerF=StandardScaler()
test['Flight Distance']=standardScalerF.fit_transform(test[['Flight Distance']])

standardScalerDD=StandardScaler()
test['Departure Delay in Minutes']=standardScalerDD.fit_transform(test[['Departure Delay in Minutes']])

standardScalerAD=StandardScaler()
test['Arrival Delay in Minutes']=standardScalerAD.fit_transform(test[['Arrival Delay in Minutes']])

test.head()

Y=test[['satisfaction']]
X=test.drop(['satisfaction'],axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

#logistic regression

claLR=LogisticRegression()
claLR.fit(X_train,Y_train)
yPredClaLR=claLR.predict(X_test)
accuracy_score(yPredClaLR, Y_test)

#knn

claKNC=KNeighborsClassifier()
claKNC.fit(X_train,Y_train)
yPredClaKNC=claKNC.predict(X_test)
accuracy_score(yPredClaKNC, Y_test)

#svm

claSVC=SVC()
claSVC.fit(X_train,Y_train)
yPredClaSVC=claSVC.predict(X_test)
accuracy_score(yPredClaSVC, Y_test)

#decision tree

claDTC=DecisionTreeClassifier()
claDTC.fit(X_train,Y_train)
yPredClaDTC=claDTC.predict(X_test)
accuracy_score(yPredClaDTC, Y_test)

#random forest

claRFC=RandomForestClassifier()
claRFC.fit(X_train,Y_train)
yPredClaRFC=claRFC.predict(X_test)
accuracy_score(yPredClaRFC, Y_test)

data=[
     ["Logistic Regression",round(accuracy_score(yPredClaLR, Y_test),3),round(f1_score(yPredClaLR, Y_test),3),round(roc_auc_score(yPredClaLR, Y_test),3)],
     ["K Nearest Neighbours Classification",round(accuracy_score(yPredClaKNC, Y_test),3),round(f1_score(yPredClaKNC, Y_test),3),round(roc_auc_score(yPredClaKNC, Y_test),3)],
     ["Support Vector Machine Classification",round(accuracy_score(yPredClaSVC, Y_test),3),round(f1_score(yPredClaSVC, Y_test),3),round(roc_auc_score(yPredClaSVC, Y_test),3)],
     ["Decision Tree Classification",round(accuracy_score(yPredClaDTC, Y_test),3),round(f1_score(yPredClaDTC, Y_test),3),round(roc_auc_score(yPredClaDTC, Y_test),3)],
     ["Random Forest Classification",round(accuracy_score(yPredClaRFC, Y_test),3),round(f1_score(yPredClaRFC, Y_test),3),round(roc_auc_score(yPredClaRFC, Y_test),3)]
    ]
columns=["Model Name","Accuracy","F1 Score","ROC AUC Score"]

print(tabulate(data, headers=columns, tablefmt="fancy_grid"))



Y=test[['satisfaction']]
X=test.drop(['satisfaction'],axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

path = "/content/drive/MyDrive/Exam results/test data"
train = pd.read_excel(path)

from sklearn.model_selection import train_test_split
X_train, x_test, Y_train, y_test = train_test_split(X, Y, test_size=00.30, random_state=101)

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)
regressor.fit(X_train, Y_train)

y_prediction = regressor.predict(x_test)
np.set_printoptions(precision=2)
y_prediction = np.array(y_pred)
y_test = np.array(y_test)

print(np.concatenate((y_pred.reshape(len(y_test),1),y_test.reshape(len(y_test),1)),1))

accuracy_score = regressor.score(X_test,y_test)
print(accuracy_score)

import math

MSE = np.square(np.subtract(y_test,y_prediction)).mean()

RMSE = math.sqrt(MSE)
print("root mean square error:\n")
print(RMSE)

import numpy

import pandas as pd

prediction_df=pd.DataFrame({'actual value':y_test,'predicted value':y_prediction,'difference':y_test-y_prediction})

y_head_rf = claRFC.predict(x_test)



from sklearn.metrics import confusion_matrix

claRFC = confusion_matrix(y_test,y_head_rf)

plt.subplot(2,3,6)
plt.title("Random Forest Confusion Matrx")
sns.heatmap(claRFC,annot=True,cmap="Blues",fmt="d",cbar=False, annot_kws={"size":24})